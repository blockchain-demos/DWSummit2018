{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote_Eth_Producer\n",
    "\n",
    "- 1. Start a remote Spark Driver - running in YARN.\n",
    "- 2. Parse all transaction logs[] from a current block on the ethereum chain\n",
    "- 3. Produce all ethereum logs to a kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Load sparkmagic \n",
    "https://github.com/jupyter-incubator/sparkmagic\n",
    "\n",
    "Sparkmagic is a set of tools for interactively working with remote Spark clusters through Livy, a Spark REST server.\n",
    "\n",
    "Any cells ran with `%%spark` will execute against a remote spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create a remote livy session\n",
    "\n",
    "Define spark configuration to use for this session.\n",
    "\n",
    "- We can leverage a **python virtual environment** in our remote Spark Session:\n",
    "  - `gethdemo.tar.gz` contains a conda virtual environment created from `./py_kafka_reqs.txt`\n",
    "  - `gethdemo.tar.gz` is available on `hdfs://user/noobie/`\n",
    "\n",
    "- Since kafka does not fully support [Delegation Tokens](https://cwiki.apache.org/confluence/display/KAFKA/KIP-48+Delegation+token+support+for+Kafka#KIP-48DelegationtokensupportforKafka-APIsandrequest/responseclasses), we can also pass in a keytab through `--files`, if connecting to a kerberized Kafka Broker.\n",
    "\n",
    "- Livy sessions can take up to 60 seconds to start. Be patient. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config \n",
    "{\n",
    "  \"name\":\"remote_eth_producer\",\n",
    "  \"driverMemory\":\"1G\",\n",
    "  \"numExecutors\":1,\n",
    "  \"proxyUser\":\"noobie\",\n",
    "  \"archives\": [\"hdfs:///user/noobie/gethdemo.tar.gz\"],\n",
    "  \"files\" : [\"hdfs:///user/noobie/noobie.keytab\"],\n",
    "  \"queue\": \"streaming\",\n",
    "  \"conf\": {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\":\"gethdemo.tar.gz/demo/bin/python3.5\",\n",
    "          \"PYSPARK_PYTHON\":\"gethdemo.tar.gz/demo/bin/python3.5\"\n",
    "          }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>290</td><td>application_1527994885375_0068</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hdp-3.demo.url:8088/proxy/application_1527994885375_0068/\">Link</a></td><td><a target=\"_blank\" href=\"http://hdp-4.demo.url:8042/node/containerlogs/container_e56_1527994885375_0068_01_000001/noobie\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s ethlogproducer -l python -u http://hdp-3.demo.url:8999 --auth Kerberos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Obtain a keberos ticket for connecting to Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "import subprocess\n",
    "kinit = '/usr/bin/kinit'\n",
    "kinit_args = [kinit, '-kt', \"noobie.keytab\" , \"noobie\"]\n",
    "subprocess.check_output(kinit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.0 Connect to Ethereum using Web3 from within the spark session\n",
    "\n",
    "Web3 is a python library for interacting with Ethereum http://web3py.readthedocs.io/en/stable/. \n",
    "Its API is derived from the [Web3.js](https://github.com/ethereum/wiki/wiki/JavaScript-API) Javascript API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest block: 5762966"
     ]
    }
   ],
   "source": [
    "%%spark -s ethlogproducer\n",
    "from web3 import Web3, HTTPProvider, IPCProvider\n",
    "\n",
    "gethRPCUrl='http://10.132.86.5:8545'\n",
    "web3 = Web3(HTTPProvider(gethRPCUrl))\n",
    "\n",
    "# Retrieve the last block number available from geth \n",
    "currentblock = web3.eth.getBlock('latest').number\n",
    "print(\"Latest block: \" + str(currentblock))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Define a HexJsonEncoder to cleanse Web3 response\n",
    "\n",
    "Web3 returns an AttributeDict containing `HexBytes`, which is not recognized by Json or Kafka. \n",
    "https://github.com/ethereum/web3.py/issues/782\n",
    "\n",
    "See cls in https://docs.python.org/2/library/json.html#basic-usage\n",
    "\n",
    "```\n",
    "usage: \n",
    "  blockjson = json.dumps(somePydDict, cls=HexJsonEncoder)    \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -s ethlogproducer\n",
    "from hexbytes import HexBytes\n",
    "import threading, logging, time, json\n",
    "\n",
    "class HexJsonEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, HexBytes):\n",
    "            return obj.hex()\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define some helper methods to pull logs[] from the eth chain\n",
    "\n",
    "#### 2.2.1 Note the schema returned from a getBlock() call\n",
    "\n",
    "- `web3.eth.getBlock(5682605,full_transactions=True).keys()`\n",
    "```\n",
    "dict_keys(['sealFields', 'mixHash', 'timestamp', 'number', 'nonce', 'gasUsed', 'gasLimit', 'size', 'totalDifficulty', 'transactions', 'extraData', 'difficulty', 'miner', 'sha3Uncles', 'transactionsRoot', 'parentHash', 'hash', 'stateRoot', 'logsBloom', 'author', 'receiptsRoot', 'uncles'])\n",
    "```\n",
    "\n",
    "#### 2.2.2 Which contains some nested fields, such as `transactions`:\n",
    "- `dict(web3.eth.getBlock(5682605,full_transactions=True)['transactions'][0]).keys()`\n",
    "```\n",
    "dict_keys(['raw', 'creates', 'condition', 'value', 'blockHash', 'gas', 'r', 'v', 'chainId', 'to', 'blockNumber', 'input', 'transactionIndex', 'standardV', 'publicKey', 'gasPrice', 's', 'nonce', 'from', 'hash'])\n",
    "```\n",
    "\n",
    "But still does not include logs which the transactions may have generated\n",
    "\n",
    "#### 2.2.3 Retrieve transaction['logs'] \n",
    "\n",
    "\n",
    "Note, the `logs` are available from a [getTransactionReceipt](https://github.com/ethereum/wiki/wiki/JSON-RPC#returns-31), but not from a `getBlock(full_transactions=True)`\n",
    "\n",
    "- `dict(web3.eth.getTransactionReceipt(transaction_hash=sample_tx)).keys()`\n",
    "```\n",
    "dict_keys(['transactionIndex', 'cumulativeGasUsed', 'root', 'logs', 'blockHash', 'logsBloom', 'status', 'transactionHash', 'blockNumber', 'contractAddress', 'gasUsed'])\n",
    "```\n",
    "\n",
    "#### 2.2.4 **logs** itself is a nested field, containing the `data` used for this transaction.\n",
    "- `dict(web3.eth.getTransactionReceipt(transaction_hash=sample_tx))['logs'].keys()`\n",
    "```\n",
    "dict_keys(['transactionIndex', 'logIndex', 'data', 'topics', 'blockHash', 'transactionHash', 'transactionLogIndex', 'type', 'blockNumber', 'address'])\n",
    "```\n",
    "\n",
    "\n",
    "Thus, we will define 2 methods:\n",
    "\n",
    "- **getTransactionsInBlock(BLOCKNUM)** \n",
    "    - Return a JSON with all transaction shown in 2.2.2 for the specified BLOCKNUM\n",
    "- **produceAllEventLogs(BLOCKNUM,GETH_EVENTS_KAFKA_TOPIC)**  \n",
    "    - Retrieves all event logs **(2.2.4)** and produces it to GETH_EVENTS_KAFKA_TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "def getTransactionsInBlock(BLOCKNUM):\n",
    "    transactions_in_range=[]\n",
    "    transactions_in_block = web3.eth.getBlock(BLOCKNUM,full_transactions=True)['transactions']     \n",
    "    for transaction in transactions_in_block:\n",
    "        if transaction is not None:\n",
    "            cleansesed_transactions=json.dumps(dict(transaction),cls=HexJsonEncoder)     \n",
    "            transactions_in_range.append(cleansesed_transactions)\n",
    "    return transactions_in_range                \n",
    "\n",
    "def produceAllEventLogs(BLOCKNUM,GETH_EVENTS_KAFKA_TOPIC):  \n",
    "    for transaction in getTransactionsInBlock(BLOCKNUM):\n",
    "        tx_event=dict(web3.eth.getTransactionReceipt(transaction_hash=json.loads(transaction)['hash']))\n",
    "        if(tx_event is not None):\n",
    "            if(tx_event['logs'] is not None and tx_event['logs']):\n",
    "                # Decode every nested tx_log in the tx_event[logs]\n",
    "                for tx_log in tx_event['logs']:\n",
    "                    tx_json=json.dumps(dict(tx_log), cls=HexJsonEncoder)\n",
    "                    producer.send(GETH_EVENTS_KAFKA_TOPIC, tx_json)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### produceAllEventLogs will produce all transactions with logs[] to a kafka topic.\n",
    "\n",
    "**sample data produced:**\n",
    "```\n",
    "{'address': '0xdd974D5C2e2928deA5F71b9825b8b646686BD200',\n",
    " 'blockHash': '0x8c11efca021f3260fab2f4736718d94acb6530a567d5462e57c484ff2e04aa3d',\n",
    " 'blockNumber': 5682604,\n",
    " 'data': '0x00000000000000000000000000000000000000000000007b1a070a274c6a8000',\n",
    " 'logIndex': 1,\n",
    " 'topics': ['0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef',\n",
    "  '0x000000000000000000000000b7fcadc9a9023b553594e607e12b40b7f8f0a670',\n",
    "  '0x0000000000000000000000002f41ea745c67724fdc65ff909318edeb73cfd6e7'],\n",
    " 'transactionHash': '0x78f7abd332f508350b8a5c5d3e0e77b4d34629059efea1ff9592a7929d311210',\n",
    " 'transactionIndex': 4,\n",
    " 'transactionLogIndex': '0x0',\n",
    " 'type': 'mined'},\n",
    " {'address': '0xdd974D5C2e2928deA5F71b9825b8b646686BD200',\n",
    " 'blockHash': '0x8c11efca021f3260fab2f4736718d94acb6530a567d5462e57c484ff2e04aa3d',\n",
    " 'blockNumber': 5682604,\n",
    " 'data': '0x00000000000000000000000000000000000000000000007cda210a234c6c5420',\n",
    " 'logIndex': 2,\n",
    " ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Kafka Producer\n",
    "Use the **HexJsonEncoder** decoder from **2.1** in the kafkaProducer **value_serializer** to perform data sanitation on producer.send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers=['hdp-4.demo.url:6667',\n",
    "                                            'hdp-5.demo.url:6667',\n",
    "                                            'hdp-6.demo.url:6667'],\n",
    "                        security_protocol=\"SASL_PLAINTEXT\",\n",
    "                        sasl_mechanism=\"GSSAPI\",\n",
    "                        value_serializer=lambda m: json.dumps(m, cls=HexJsonEncoder).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Smoketest producing 1 block's event logs to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer-metrics': {'metadata-age': 2.923041259765625, 'batch-size-avg': 1052.0108695652175, 'io-ratio': 0.001169609662193618, 'record-size-avg': 720.9565217391304, 'io-time-ns-avg': 145312.07534502138, 'response-rate': 2.8647041684510257, 'bufferpool-wait-ratio': 0.0, 'batch-size-max': 11493.0, 'byte-rate': 2939.8883841457987, 'request-rate': 2.8357456660087172, 'incoming-byte-rate': 336.30373047826953, 'record-queue-time-avg': 0.0029542083325593367, 'connection-creation-rate': 0.05788735491010056, 'record-send-rate': 3.8880908137602885, 'connection-count': 1.0, 'io-wait-ratio': 0.09714018325392146, 'compression-rate-avg': 1.0, 'network-io-rate': 5.700440358650351, 'outgoing-byte-rate': 3010.6253046303113, 'record-queue-time-max': 0.00562286376953125, 'requests-in-flight': 0.0, 'request-size-max': 11569.0, 'produce-throttle-time-avg': 0.0, 'produce-throttle-time-max': 0.0, 'record-retry-rate': 0.0, 'request-latency-max': 101.8209457397461, 'select-rate': 8.04884422848802, 'request-size-avg': 1061.6530612244899, 'records-per-request-avg': 1.391304347826087, 'record-size-max': 1249.0, 'io-wait-time-ns-avg': 12068852.838480247, 'connection-close-rate': 0.02895032544696359, 'record-error-rate': 0.0, 'request-latency-avg': 2.054204746168487}, 'kafka-metrics-count': {'count': 56.0}, 'producer-node-metrics.node-1003': {'request-size-avg': 1082.875, 'request-rate': 2.779395609533765, 'outgoing-byte-rate': 3009.7227035167684, 'response-rate': 2.779430265143889, 'request-latency-max': 101.8209457397461, 'request-size-max': 11569.0, 'request-latency-avg': 2.0250827074050903, 'incoming-byte-rate': 250.03017667831085}, 'producer-node-metrics.node-bootstrap': {'request-rate': 0.05787228939286981, 'request-size-avg': 43.0, 'outgoing-byte-rate': 2.488546593163211, 'incoming-byte-rate': 86.4040587129231, 'request-latency-max': 6.540536880493164, 'request-size-max': 45.0, 'response-rate': 0.08680913811425713, 'request-latency-avg': 3.4520626068115234}, 'producer-topic-metrics.eth_eventlogs': {'record-send-rate': 3.8880401533170224, 'byte-rate': 2939.860456219719, 'record-error-rate': 0.0, 'record-retry-rate': 0.0, 'compression-rate': 1.0}}"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "kafkatopic=\"eth_eventlogs\"\n",
    "\n",
    "currentblock = web3.eth.getBlock('latest').number\n",
    "produceAllEventLogs(BLOCKNUM= currentblock,\n",
    "                    GETH_EVENTS_KAFKA_TOPIC = kafkatopic )\n",
    "\n",
    "# Print metrics to verify producer connected successfuly \n",
    "producer.metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run a producer for a given number of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at block: 5762965\n",
      "Finished producing block :5762968"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import sys\n",
    "\n",
    "blockstart= web3.eth.getBlock('latest').number-1\n",
    "blockend  = web3.eth.getBlock('latest').number+5000\n",
    "\n",
    "kafkatopic=\"eth_eventlogs\"\n",
    "\n",
    "print(\"Start at block: \" + str(blockstart))\n",
    "try:\n",
    "    global blockstart\n",
    "    while blockstart < blockend:\n",
    "        currentblock = web3.eth.getBlock('latest').number\n",
    "        if currentblock < blockstart:\n",
    "            time.sleep(0.2)\n",
    "            pass\n",
    "        else:\n",
    "            produceAllEventLogs(BLOCKNUM= currentblock,\n",
    "                                GETH_EVENTS_KAFKA_TOPIC = kafkatopic ) \n",
    "            blockstart=blockstart+1\n",
    "            time.sleep(0.2)               \n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "    pass\n",
    "print(\"Finished producing block :\" + str(blockend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When done, cleanup livy sessions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark delete -s ethlogproducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark cleanup\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
